{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models in PyMC3\n",
    "\n",
    "Bayesian inference begins with specification of a probability model relating unknown variables to data. PyMC3 provides the basic building blocks for Bayesian probability models: \n",
    "\n",
    "1. stochastic random variables\n",
    "2. deterministic variables\n",
    "3. factor potentials. \n",
    "\n",
    "A **stochastic random variable** is a factor whose value is not completely determined by its parents, while the value of a **deterministic random variable** is entirely determined by its parents. Most models can be constructed using only these two variable types. The third quantity, the **factor potential**, is *not* a variable but simply a\n",
    "log-likelihood term or constraint that is added to the joint log-probability to modify it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Inferring patterns in UK coal mining disasters\n",
    "\n",
    "To motivate this section, let's model a different dataset: a time series of recorded coal mining \n",
    "disasters in the UK from 1851 to 1962.\n",
    "\n",
    "Occurrences of disasters in the time series is thought to be derived from a \n",
    "Poisson process with a large rate parameter in the early part of the time \n",
    "series, and from one with a smaller rate in the later part. We are interested \n",
    "in locating the change point in the series, which perhaps is related to changes \n",
    "in mining safety regulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "year = np.arange(1851, 1962)\n",
    "disasters_data = np.array([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n",
    "                         3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n",
    "                         2, 2, 3, 4, 2, 1, 3, 2, 2, 1, 1, 1, 1, 3, 0, 0,\n",
    "                         1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n",
    "                         0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n",
    "                         3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n",
    "                         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12.5, 3.5))\n",
    "n_count_data = len(disasters_data)\n",
    "ax.bar(year, disasters_data, color=\"#348ABD\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Disasters\")\n",
    "ax.set_title(\"UK coal mining disasters, 1851-1962\")\n",
    "ax.set_xlim(1851, 1962);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use Poisson random variables for this type of count data. Denoting year $i$'s accident count by $y_i$, \n",
    "\n",
    "$$ y_i \\sim \\text{Poisson}(\\lambda)  $$\n",
    "\n",
    "The modeling problem revolves around estimating the values of the $\\lambda$ parameters. Looking at the time series above, it appears that the rate declines later in the time series.\n",
    "\n",
    "A ***changepoint model*** identifies a point (year) during the observation period (call it $\\tau$) after which the parameter $\\lambda$ drops to a lower value. So we are estimating two $\\lambda$ parameters: one for the early period and another for the late period.\n",
    "\n",
    "$$\n",
    "\\lambda = \n",
    "\\begin{cases}\n",
    "\\lambda_1  & \\text{if } t \\lt \\tau \\cr\n",
    "\\lambda_2 & \\text{if } t \\ge \\tau\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We need to assign prior probabilities to both $\\lambda$ parameters. The gamma distribution not only provides a continuous density function for positive numbers, but it is also **conjugate** with the Poisson sampling distribution. We will specify suitably vague hyperparameters $\\alpha$ and $\\beta$ for both priors.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\lambda_1 &\\sim \\text{Gamma}( \\alpha, \\beta ) \\cr\n",
    "\\lambda_2 &\\sim \\text{Gamma}( \\alpha, \\beta )\n",
    "\\end{aligned}$$\n",
    "\n",
    "Since we do not have any intuition about the location of the changepoint (prior to viewing the data), we will assign a discrete uniform prior over all years 1851-1962.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\tau \\sim \\text{DiscreteUniform(1851,1962) }\\cr\n",
    "& \\Rightarrow P( \\tau = k ) = \\frac{1}{111}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FreeRV class\n",
    "\n",
    "A stochastic variable is represented in PyMC3 by a `FreeRV` class. This structure adds functionality to Theano's `TensorVariable` class, by mixing in the PyMC `Factor` class. A `Factor` is used whenever a variable contributes a log-probability term to a model. Hence, you know a variable is a subclass of `Factor` whenever it has a `logp` method, as we saw in the previous section.\n",
    "\n",
    "A `FreeRV` object has several important attributes:\n",
    "\n",
    "`dshape`\n",
    ":   The variable's shape.\n",
    "\n",
    "`dsize`\n",
    ":   The overall size of the variable.\n",
    "\n",
    "`distribution`\n",
    ":   The probability density or mass function that describes the distribution of the variable's values.\n",
    "\n",
    "`logp`\n",
    ":   The log-probability of the variable's current value given the values\n",
    "    of its parents.\n",
    "\n",
    "`init_value`\n",
    ":   The initial value of the variable, used by many algorithms as a starting point for model fitting.\n",
    "\n",
    "`model`\n",
    ":   The PyMC model to which the variable belongs.\n",
    "\n",
    "\n",
    "### Creation of stochastic random variables\n",
    "\n",
    "There are two ways to create stochastic random variables (`FreeRV` objects), which we will call the **automatic**, and **manual** interfaces.\n",
    "\n",
    "#### Automatic\n",
    "\n",
    "Stochastic random variables with standard distributions provided by PyMC3 can be created in a single line using special subclasses of the `Distribution` class. For example, as we have seen, the uniformly-distributed discrete variable $switchpoint$ in the coal mining disasters model is created using the automatic interface as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Model, Uniform\n",
    "\n",
    "with Model() as disaster_model:\n",
    "\n",
    "    switchpoint = Uniform('switchpoint', lower=0, upper=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the rate parameters can automatically be given exponential priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Exponential\n",
    "\n",
    "with disaster_model:\n",
    "    early_mean = Exponential('early_mean', lam=1)\n",
    "    late_mean = Exponential('late_mean', lam=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC includes most of the probability density functions (for continuous variables) and probability mass functions (for discrete variables) used in statistical modeling. Continuous variables are represented by a specialized subclass of `Distribution` called `Continuous` and discrete variables by the `Discrete` subclass.\n",
    "\n",
    "The main differences between these two sublcasses are in the `dtype` attribute (`int64` for `Discrete` and `float64` for `Continuous`) and the `defaults` attribute, which determines which summary statistic to use for initial values when one is not specified ('mode' for `Discrete` and 'median', 'mean', and 'mode' for `Continuous`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('median', 'mean', 'mode')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switchpoint.distribution.defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we previewed in the introduction, `Distribution` has a class method `dist` that returns a probability distribution of that type, without being wrapped in a PyMC random variable object. Sometimes we wish to use a particular statistical distribution, without using it as a variable in a model; for example, to generate random numbers from the distribution. This class method allows that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\text{None} \\sim \\text{Exponential}(\\mathit{lam}=1.0)$"
      ],
      "text/plain": [
       "<pymc3.distributions.continuous.Exponential at 0x7fcdbb9e3f60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exponential.dist(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual\n",
    "\n",
    "The uniformly-distributed discrete stochastic variable `switchpoint` in the disasters model could alternatively be created from a function that computes its log-probability as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import DensityDist\n",
    "from pymc3.math import switch\n",
    "\n",
    "with Model():\n",
    "    \n",
    "    def uniform_logp(value, lower=0, upper=111):\n",
    "        \"\"\"The switchpoint for the rate of disaster occurrence.\"\"\"\n",
    "        return switch((value > upper) | (value < lower), -np.inf, -np.log(upper - lower + 1))\n",
    "\n",
    "    switchpoint = DensityDist('switchpoint', logp=uniform_logp, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-4.71849887)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switchpoint.logp({'switchpoint':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-4.71849887)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switchpoint.logp({'switchpoint': 44})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-inf)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switchpoint.logp({'switchpoint':-1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things to notice: while the function specified for the `logp` argument can be an arbitrary Python function, it must use **Theano operators and functions** in its body. This is because one or more of the arguments passed to the function may be `TensorVariables`, and they must be supported. Also, we passed the value to be evaluated by the `logp` function as a **dictionary**, rather than as a plain integer. By convention, values in PyMC3 are passed around as a data structure called a `Point`. Points in parameter space are represented by dictionaries with parameter names as they keys and the value of the parameters as the values.\n",
    "\n",
    "To emphasize, the Python function passed to `DensityDist` should compute the *log*-density or *log*-probability of the variable. That is why the return value in the example above is `-log(upper-lower+1)` rather than `1/(upper-lower+1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ObservedRV Class\n",
    "\n",
    "Stochastic random variables whose values are observed (*i.e.* data likelihoods) are represented by a different class than unobserved random variables. A `ObservedRV` object is instantiated any time a stochastic variable is specified with data passed as the `observed` argument. \n",
    "\n",
    "Otherwise, observed stochastic random variables are created via the same interfaces as unobserved: **automatic** or **manual**. As an example of an automatic instantiation, consider a Poisson data likelihood :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Poisson\n",
    "\n",
    "with disaster_model:\n",
    "    \n",
    "    disasters = Poisson('disasters', mu=3, observed=[3,4,1,2,0,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen manual instantiation, from the melanoma survial model where the exponential survival likelihood was implemented manually:\n",
    "\n",
    "```python\n",
    "def logp(failure, value):\n",
    "    return (failure * log(lam) - lam * value).sum()\n",
    "\n",
    "x = DensityDist('x', logp, observed={'failure':failure, 'value':t})\n",
    "```\n",
    "\n",
    "Notice in this example that there are two vetors observed data for the likelihood `x`, passed as a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important responsibility of `ObservedRV` is to automatically handle missing values in the data, when they are present (absent?). More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Variables\n",
    "\n",
    "A deterministic variable is one whose values are **completely determined** by the values of their parents. For example, in our disasters model, `rate` is a deterministic variable.\n",
    "\n",
    "```python\n",
    "with disaster_model:\n",
    "    \n",
    "    rate = pm.Deterministic('rate', switch(switchpoint >= np.arange(112), early_mean, late_mean))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so `rate`'s value can be computed exactly from the values of its parents `early_mean`, `late_mean` and `switchpoint`.\n",
    "\n",
    "There are two types of deterministic variables in PyMC3\n",
    "\n",
    "#### Anonymous deterministic variables\n",
    "\n",
    "The easiest way to create a deterministic variable is to operate on or transform one or more variables in a model directly. For example, the simplest way to specify the `rate` variable above is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with disaster_model:\n",
    "    \n",
    "    rate = switch(switchpoint >= np.arange(112), early_mean, late_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, let's say we wanted to use the mean of the `early_mean` and `late_mean` variables somehere in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with disaster_model:\n",
    "    \n",
    "    mean_of_means = (early_mean + late_mean)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are called *anonymous* variables because we did not wrap it with a call to `Determinstic`, which gives it a name as its first argument. We simply specified the variable as a Python (or, Theano) expression. This is therefore the simplest way to construct a determinstic variable. The only caveat is that the values generated by anonymous determinstics at every iteration of a MCMC algorithm, for example, are not recorded to the resulting trace. So, this approach is only appropriate for intermediate values in your model that you do not wish to obtain posterior estimates for, alongside the other variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named deterministic variables\n",
    "\n",
    "To ensure that deterministic variables' values are accumulated during sampling, they should be instantiated using the **named deterministic** interface; this uses the `Deterministic` function to create the variable. Two things happen when a variable is created this way:\n",
    "\n",
    "1. The variable is given a name (passed as the first argument)\n",
    "2. The variable is appended to the model's list of random variables, which ensures that its values are tallied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Deterministic\n",
    "\n",
    "with disaster_model:\n",
    "    \n",
    "    rate = Deterministic('rate', switch(switchpoint >= np.arange(112), early_mean, late_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'switchpoint_interval__': switchpoint_interval__,\n",
       " 'switchpoint': switchpoint,\n",
       " 'early_mean_log__': early_mean_log__,\n",
       " 'early_mean': early_mean,\n",
       " 'late_mean_log__': late_mean_log__,\n",
       " 'late_mean': late_mean,\n",
       " 'disasters': disasters,\n",
       " 'rate': rate}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster_model.named_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Potentials\n",
    "\n",
    "For some applications, we want to be able to modify the joint density by incorporating terms that don't correspond to probabilities of variables conditional on parents, for example:\n",
    "\n",
    "$$p(x_0, x_2, \\ldots x_{N-1}) \\propto \\prod_{i=0}^{N-2} \\psi_i(x_i, x_{i+1})$$\n",
    "\n",
    "In other cases we may want to add probability terms to existing models. For example, suppose we want to constrain the difference between the early and late means in the disaster model to be less than 1, so that the joint density becomes: \n",
    "\n",
    "$$p(y,\\tau,\\lambda_1,\\lambda_2) \\propto p(y|\\tau,\\lambda_1,\\lambda_2) p(\\tau) p(\\lambda_1) p(\\lambda_2) I(|\\lambda_2-\\lambda_1| \\lt 1)$$\n",
    "\n",
    "We call such log-probability terms **factor potentials** (Jordan 2004). Bayesian\n",
    "hierarchical notation doesn't accomodate these potentials. \n",
    "\n",
    "### Creation of Potentials\n",
    "\n",
    "A potential can be created via the `Potential` function, in a way very similar to `Deterministic`'s named interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Potential\n",
    "\n",
    "with disaster_model:\n",
    "    \n",
    "    rate_constraint = Potential('rate_constraint', switch((late_mean - early_mean)>0, -np.inf, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes just a `name` as its first argument and an expression returning the appropriate log-probability as the second argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with MCMC\n",
    "\n",
    "PyMC's core business is using Markov chain Monte Carlo to fit virtually any probability model. This involves the assignment and coordination of a suite of **step methods**, each of which is responsible for updating one or more variables. \n",
    "\n",
    "The user's interface to PyMC's sampling algorithms is the `sample` function:\n",
    "\n",
    "```python\n",
    "sample(draws, step=None, start=None, trace=None, chain=0, njobs=1, tune=None, \n",
    "        progressbar=True, model=None, random_seed=None)\n",
    "```\n",
    "\n",
    "`sample` assigns particular samplers to model variables, and generates samples from them. The `draws` argument\n",
    "controls the total number of MCMC iterations. PyMC can automate most of the details of sampling, outside of the selection of the number of draws, using default settings for several parameters that control how the sampling is set up and conducted. However, users may manually intervene in the specification of the sampling by passing values to a number of keyword argumetns for `sample`.\n",
    "\n",
    "### Assigning step methods\n",
    "\n",
    "The `step` argument allows users to assign a MCMC sampling algorithm to the entire model, or to a subset of the variables in the model. For example, if we wanted to use the Metropolis-Hastings sampler to fit our model, we could pass an instance of that step method to `sample` via the `step` argument:\n",
    "\n",
    "```python\n",
    "with my_model:\n",
    "\n",
    "    trace = sample(1000, step=Metropolis())\n",
    "```\n",
    "\n",
    "or if we only wanted to assign `Metropolis` to a parameter called `β`:\n",
    "\n",
    "```python\n",
    "with my_model:\n",
    "\n",
    "    trace = sample(1000, step=Metropolis(vars=[β]))\n",
    "```\n",
    "\n",
    "When `step` is not specified by the user, PyMC3 will assign step methods to variables automatically. To do so, each step method implements a class method called `competence`. This method returns a value from 0 (incompatible) to 3 (ideal), based on the attributes of the random variable in question. `sample` assigns the step method that returns the highest competence value to each of its unallocated stochastic random variables. In general:\n",
    "\n",
    "* Binary variables will be assigned to `BinaryMetropolis` (Metropolis-Hastings for binary values)\n",
    "* Discrete variables will be assigned to `Metropolis`\n",
    "* Continuous variables will be assigned to `NUTS` (No U-turn Sampler)\n",
    "\n",
    "### Starting values\n",
    "\n",
    "The `start` argument allows for the specification of starting values for stochastic random variables in the model. MCMC algorithms begin by initializing all unknown quantities to arbitrary starting values. Though in theory the value can be any value under the support of the distribution describing the random variable, we can make sampling more difficult if an initial value is chosen in the extreme tail of the distribution, for example. If starting values are not passed by the user, default values are chosen from the mean, median or mode of the distribution.\n",
    "\n",
    "As suggested in the previous section on approximation methods, it is sometimes useful to initialize a MCMC simulation at the maximum *a posteriori* (MAP) estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Model() as disaster_model:\n",
    "\n",
    "    switchpoint = Uniform('switchpoint', lower=year.min(), upper=year.max())\n",
    "    early_mean = Exponential('early_mean', lam=0.5)\n",
    "    late_mean = Exponential('late_mean', lam=0.5)\n",
    "\n",
    "    rate = switch(switchpoint >= year, early_mean, late_mean)\n",
    "    \n",
    "    disasters = Poisson('disasters', rate, observed=disasters_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fonnesbeck/anaconda3/envs/mcmc_tutorial/lib/python3.7/site-packages/pymc3/tuning/starting.py:61: UserWarning: find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.\n",
      "  warnings.warn('find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.')\n",
      "logp = -213.77, ||grad|| = 68.219: 100%|██████████| 9/9 [00:00<00:00, 754.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from pymc3 import find_MAP\n",
    "\n",
    "with disaster_model:\n",
    "    \n",
    "    start = find_MAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'switchpoint_interval__': array(0.),\n",
       " 'early_mean_log__': array(0.91451801),\n",
       " 'late_mean_log__': array(-0.1043578),\n",
       " 'switchpoint': array(1906.),\n",
       " 'early_mean': array(2.49557212),\n",
       " 'late_mean': array(0.9009029)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 100 samples in chain.\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [late_mean]\n",
      ">Metropolis: [early_mean]\n",
      ">Metropolis: [switchpoint]\n",
      "Sampling 2 chains: 100%|██████████| 1200/1200 [00:00<00:00, 1748.18draws/s]\n",
      "The number of effective samples is smaller than 25% for some parameters.\n"
     ]
    }
   ],
   "source": [
    "from pymc3 import sample, Metropolis\n",
    "\n",
    "with disaster_model:\n",
    "    trace = sample(100, step=Metropolis(), cores=2, start=start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are sampling more than one Markov chain from our model, it is often recommended to initialize each chain to different starting values, so that lack of convergence can be more easily detected (see *Model Checking* section). \n",
    "\n",
    "### Storing samples\n",
    "\n",
    "Notice in the above call to `sample` that output is assigned to a variable we have called `trace`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MultiTrace: 2 chains, 100 iterations, 6 variables>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `MultiTrace` object is a data structure that stores the samples from an MCMC run in a tabular structure. By default, `sample` will create a new `MultiTrace` object that stores its samples in memory, as a NumPy `ndarray`. We can override the default behavior by specifying the `trace` argument. There are three options:\n",
    "\n",
    "1. Selecting an alternative database backend to keeping samples in an `ndarray`. Passing either \"text\" or \"sqlite\", for example, will save samples to text files or a SQLite database, respectively. An instance of a backend can also be passed.\n",
    "2. Passing a list of variables will only record samples for the subset of variables specified in the list. These will be stored in memory.\n",
    "3. An existing `MultiTrace` object. This will add samples to an existing backend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 100 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [late_mean, early_mean, switchpoint]\n",
      "Sampling 2 chains: 100%|██████████| 200/200 [00:00<00:00, 1138.51draws/s]\n",
      "The chain contains only diverging samples. The model is probably misspecified.\n",
      "The acceptance probability does not match the target. It is 0.0, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The chain contains only diverging samples. The model is probably misspecified.\n",
      "The acceptance probability does not match the target. It is 0.0, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n"
     ]
    }
   ],
   "source": [
    "with disaster_model:\n",
    "    db_trace = sample(100, tune=0, cores=2, trace='sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm mcmc.sqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel sampling\n",
    "\n",
    "Nearly all modern desktop computers have multiple CPU cores, and running multiple MCMC chains is an **embarrasingly parallel** computing task. It is therefore relatively simple to run chains in parallel in PyMC3. This is done by setting the `cores` argument in `sample` to some value between 2 and the number of cores on your machine (you can specify more chains than cores, but you will not gain efficiency by doing so). The default value of `cores` is `None`, which will select the number of CPUs on your machine, to a maximum of 4. \n",
    "\n",
    "> Keep in mind that some chains might themselves be multithreaded via openmp or BLAS. In those cases it might be faster to set this to 1.\n",
    "\n",
    "By default, PyMC3 will run a sample a minimum of 2 and a maximum of `cores` chains. However, the number of chains sampled can be set independently of the number of cores by specifying the `chains` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 100 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 2 jobs)\n",
      "NUTS: [late_mean, early_mean, switchpoint]\n",
      "Sampling 4 chains: 100%|██████████| 800/800 [01:00<00:00,  6.07draws/s]\n",
      "The acceptance probability does not match the target. It is 0.5286333447366897, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.5574995647709398, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.5617403175489052, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.5282589144712533, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The number of effective samples is smaller than 25% for some parameters.\n"
     ]
    }
   ],
   "source": [
    "with disaster_model:\n",
    "    ptrace = sample(100, tune=100, chains=4, cores=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running $n$ iterations with $c$ chains will result in $n \\times c$ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptrace['early_mean'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to specify different arguments for each chain, a list of argument values can be passed to `sample` as appropriate. For example, if we want to initialize random variables to particular (*e.g.* dispersed) values, we can pass a list of dictionaries to `start`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 10 samples in chain.\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [late_mean, early_mean, switchpoint]\n",
      "Sampling 2 chains: 100%|██████████| 220/220 [00:07<00:00, 28.22draws/s]\n",
      "/home/fonnesbeck/anaconda3/envs/mcmc_tutorial/lib/python3.7/site-packages/pymc3/sampling.py:464: UserWarning: The number of samples is too small to check convergence reliably.\n",
      "  warnings.warn(\"The number of samples is too small to check convergence reliably.\")\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.4240191476955976, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n"
     ]
    }
   ],
   "source": [
    "with disaster_model:\n",
    "    ptrace = sample(10, tune=100, cores=2, discard_tuned_samples=False, init=None,\n",
    "                    start=[{'early_mean':0.1}, {'early_mean':10}]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.38481639, 1.38481639, 4.46040266, 4.53289822, 2.81074373]),\n",
       " array([10.        , 10.        ,  5.35720418,  4.57184243,  3.34497226])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[chain[:5] for chain in ptrace.get_values('early_mean', combine=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating several chains is generally recommended because it aids in model checking, allowing statistics such as the potential scale reduction factor ($\\hat{R}$) and effective sample size to be calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step methods\n",
    "\n",
    "Step method classes handle individual stochastic variables, or sometimes groups of them. They are responsible for making the variables they handle take **single MCMC steps** conditional on the rest of the model. Each PyMC step method (usually subclasses of `ArrayStep`) implements a method called `astep()`, which is called iteratively by `sample`. \n",
    "\n",
    "All step methods share an optional argument `vars` that allows a particular subset of variables to be handled by the step method instance. Particular step methods will have additional arguments for setting parameters and preferences specific to that sampling algorithm.\n",
    "\n",
    "> NB: when a PyMC function or method has an argument called `vars` it is expecting a list of variables (*i.e.* the variables themselves), whereas arguments called `varnames` expect a list of variables names (*i.e.* strings)\n",
    "\n",
    "### HamiltonianMC\n",
    "\n",
    "The Hamiltonian Monte Carlo algorithm is implemented in the `HamiltonianMC` class. Being a gradient-based sampler, it is only suitable for **continuous random variables**. Several optional arguments can be provided by the user. The algorithm is **non-adaptive**, so the parameter values passed at instantiation are fixed at those values throughout sampling.\n",
    "\n",
    "`HamiltonianMC` requires a scaling matrix parameter `scaling`, which is analogous to the variance parameter for the jump proposal distribution in Metropolis-Hastings, although it is used somewhat differently here. The matrix gives an approximate shape of the posterior distribution, so that `HamiltonianMC` does not make jumps that are too large in some directions and too small in other directions. It is important to set this scaling parameter to a reasonable value to facilitate efficient sampling. This is especially true for models that have many unobserved stochastic random variables or models with highly non-normal posterior distributions. \n",
    "\n",
    "Fortunately, `HamiltonianMC` can often make good guesses for the scaling parameters. If you pass a point in parameter space (as a dictionary of variable names to parameter values, the same format as returned by `find_MAP`), it will look at the **local curvature** of the log posterior-density (the diagonal of the Hessian matrix) at that point to guess values for a good scaling vector, which can result in a good scaling value. Also, the MAP estimate is often a good point to use to initiate sampling. \n",
    "\n",
    "- `scaling` \n",
    ": Scaling for momentum distribution. If a 1-dimensional array is passed, it is interpreted as a matrix diagonal.\n",
    "            \n",
    "- `step_scale` \n",
    ": Size of steps to take, automatically scaled down by $1/n^{0.25}$. Defaults to .25.\n",
    "            \n",
    "- `path_length` \n",
    ": total length to travel during leapfrog. Defaults to 2.\n",
    "            \n",
    "- `is_cov` \n",
    ": Flag for treating scaling as a covariance matrix/vector, if True. Treated as precision otherwise.\n",
    "            \n",
    "- `step_rand` \n",
    ": A function which takes the step size and returns an new one used to randomize the step size at each iteration.\n",
    "\n",
    "\n",
    "### NUTS\n",
    "\n",
    "A disadgantage of the HMC sampler is that there are key hyperparameters that require tuning for sampling to proceed efficiently. Hoffman and Gelman (2014) developed an auto-tuning variant of HMC that takes care of selecting path lengths and step sizes.\n",
    "\n",
    "`NUTS` is the No U-turn Sampler of Hoffman and Gelman (2014), an adaptive version of Hamiltonian MC that **automatically tunes** the step size and number on the fly. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution. True to its name, it stops automatically when it starts to double back and retrace its steps.\n",
    "\n",
    "The algorithm employs **binary doubling**, which takes leapfrog steps alternating in direction with respect to the initial gradient. That is, one step is taken in the forward direction, two in the reverse direction, then four, eight, etc. The result is a balanced, binary tree with nodes comprised of Hamiltonian states. \n",
    "\n",
    "![](images/binary_doubling.png)\n",
    "\n",
    "Doubling process builds a balanced binary tree whose leaf nodes correspond to\n",
    "position-momentum states. Doubling is halted when the subtrajectory from the\n",
    "leftmost to the rightmost nodes of any balanced subtree of the overall binary tree starts to double back on itself\n",
    "\n",
    "![](images/uturn.png)\n",
    "\n",
    "To ensure detailed balance, a slice variable is sampled from:\n",
    "\n",
    "$$ u \\sim \\text{Uniform}(0, \\exp[L(\\theta) - 0.5 r \\cdot r])$$\n",
    "\n",
    "where $r$ is the initial momentum vector. The next sample is then chosen uniformly from the points in the remaining balanced tree.\n",
    "\n",
    "In addition to the arguments to `HamiltonianMC`, `NUTS` takes additional parameters to controls the tuning. The most important of these is the target acceptance rate for the Metropolis acceptance phase of the algorithm, `taget_accept`. \n",
    "Sometimes if the NUTS struggles to sample efficiently, changing this parameter above the default target rate of 0.8 will improve sampling (the original recommendation by Hoffman & Gelman was 0.6). Increasing the rate very high will also make the sampler more conservative, however, taking many small steps at every iteration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 100 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [late_mean, early_mean, switchpoint]\n",
      "Sampling 2 chains: 100%|██████████| 600/600 [00:36<00:00,  7.88draws/s]\n",
      "The acceptance probability does not match the target. It is 0.9665921540379762, but should be close to 0.99. Try to increase the number of tuning steps.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The chain reached the maximum tree depth. Increase max_treedepth, increase target_accept or reparameterize.\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The number of effective samples is smaller than 10% for some parameters.\n"
     ]
    }
   ],
   "source": [
    "with disaster_model:\n",
    "    trace_99 = sample(100, tune=200, cores=2, target_accept=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is rarely a reason to use `HamiltonianMC` rather than `NUTS`. It is the default sampler for continuous variables in PyMC3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis\n",
    "\n",
    "``Metropolis`` implements a Metropolis-Hastings step, as described the theory section, and is designed to handle float- and integer-valued variables.\n",
    "\n",
    "A `Metropolis` step method can be instantiated with any of several optional arguments:\n",
    "\n",
    "\n",
    "- `S`\n",
    ":   This sets the proposal standard deviation or covariance matrix.\n",
    "\n",
    "- `proposal_dist`\n",
    ":   A function that generates zero-mean random deviates used as proposals. Defaults to the normal distribution.\n",
    "\n",
    "- `scaling`\n",
    ":   An initial scale factor for the proposal\n",
    "\n",
    "- `tune_interval`\n",
    ":   The number of intervals between tuning updates to `scaling` factor.\n",
    "\n",
    "When the step method is instantiated, the `proposal_dist` is parameterized with the value passed for `S`. While sampling, the value of `scaling` is used to scale the value proposed by `proposal_dist`, and this value is tuned throughout the MCMC run. During tuning, the acceptance ratio of the step method is examined, and this scaling factor\n",
    "is updated accordingly. Tuning only occurs when the acceptance rate is **lower than 20%** or **higher than 50%**; rates between 20-50% are considered optimal for Metropolis-Hastings sampling. The default tuning interval (`tune_interval`) is 100 iterations.\n",
    "\n",
    "Although tuning will continue throughout the sampling loop, it is important to verify that the\n",
    "**diminishing tuning** condition of [Roberts and Rosenthal (2007)](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.jap/1183667414) is satisfied: the\n",
    "amount of tuning should decrease to zero, or tuning should become very infrequent.\n",
    "\n",
    "`Metropolis` handles discrete variable types automatically by rounding the proposed values and casting them to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BinaryMetropolis\n",
    "\n",
    "While binary (boolean) variables can be handled by the `Metropolis` step method, sampling will be very inefficient. The `BinaryMetropolis` class is optimized to handle binary variables, by one of only two possible values. The only tuneable parameter is the `scaling` argument, which is used to vary the Bernoulli probability:\n",
    "\n",
    "    p_jump = 1. - .5 ** self.scaling\n",
    "\n",
    "This value is compared to pseudo-random numbers generated by the step method, to determine whether a 0 or 1 is proposed.\n",
    "\n",
    "`BinaryMetropolis` will be automatically selected for random variables that are distributed as Bernoulli, or categorical with only 2 categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice\n",
    "\n",
    "Though the Metropolis-Hastings algorithm is easy to implement for a variety of models, its efficiency is poor. We have seen that it is possible to tune Metropolis samplers, but it would be nice to have a \"black-box\" method that works for arbitrary continuous distributions, which we may know little about a priori.\n",
    "\n",
    "The **slice sampler** (Neal 2003) improves upon the Metropolis sampler by being both efficient and easy to program generally. The idea is to first sample from the conditional distribution for $y$ (i.e., $Pr(x)$) given some current value of $x$, which is uniform over the $(0,f(x))$, and conditional on this value for $y$, then sample $x$, which is uniform on $S = {x : y < f (x)}$.\n",
    "\n",
    "The steps required to perform a single iteration of the slice sampler to update the current value of $x_i$ is as follows:\n",
    "\n",
    "1. Sample $y$ uniformly on (0,f(xi)).\n",
    "2. Use this value $y$ to define a horizontal *slice* $S = {x : y < f (x)}$.\n",
    "3. Establish an interval, I=(xa,xb), around xi that contains most of the slice.\n",
    "4. Sample $x_{i+1}$ from the region of the slice overlaping I.\n",
    "\n",
    "Hence, slice sampling employs an **auxilliary variable** ($y$) that is not retained at the end of the iteration. Note that in practice one may operate on the log scale such that $g(x) = \\log(f (x))$ to avoid floating-point underflow. In this case, the auxiliary variable becomes $z = log(y) = g(x_i) − e$, where $e \\sim \\text{Exp}(1)$, resulting in the slice $S = \\{x : z < g(x)\\}$.\n",
    "\n",
    "There are many ways of establishing and sampling from the interval $I$, with the only restriction being that the resulting Markov chain leaves $f(x)$ **invariant**. The objective is to include as much of the slice as possible, so that the potential step size can be large, but not (much) larger than the slice, so that the sampling of invalid points is minimized. Ideally, we would like it to be the slice itself, but it may not always be feasible to determine (and certainly not automatically).\n",
    "\n",
    "In PyMC3, the `Slice` class implements the **univariate** slice sampler. It is suitable for univariate, continuous variables. There is a single user-defined parameter `w`, which sets the width of the initial slice. If not specified, it defaults to a width of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "CompoundStep\n",
      ">Slice: [late_mean]\n",
      ">Slice: [early_mean]\n",
      ">Slice: [switchpoint]\n",
      "Sampling 2 chains: 100%|██████████| 5000/5000 [00:07<00:00, 705.79draws/s]\n"
     ]
    }
   ],
   "source": [
    "from pymc3 import Slice\n",
    "\n",
    "with disaster_model:\n",
    "    \n",
    "    slice_trace = sample(2000, cores=2, step=Slice())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_trace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-aaefb0d0dec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'early_mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'late_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_trace' is not defined"
     ]
    }
   ],
   "source": [
    "plot_trace(slice_trace, var_names=['early_mean','late_mean']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## To Learn More\n",
    "\n",
    "- Hoffman MD, Gelman A. 2014. The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. The Journal of Machine Learning Research. 15(1):1593-1623.\n",
    "- M.I. Jordan. 2004. Graphical models. Statist. Sci., 19(1):140–155.\n",
    "- Neal, R. M. 2003. Slice sampling. The Annals of Statistics, 31(3), 705–767. doi:10.1111/1467-9868.00198"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
